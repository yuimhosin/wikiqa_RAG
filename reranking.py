# reranking.py - é‡æ’åºæ¨¡å—

import re
from typing import List, Tuple, Any
import logging

logger = logging.getLogger(__name__)


class SimpleReranker:
    """
    ç®€å•é‡æ’åºå™¨ - åŸºäºå…³é”®è¯åŒ¹é…å’ŒåŸºç¡€å¯å‘å¼
    """

    def __init__(self, keyword_weight=0.7, length_weight=0.3):
        """
        åˆå§‹åŒ–ç®€å•é‡æ’åºå™¨

        Args:
            keyword_weight: å…³é”®è¯åŒ¹é…æƒé‡
            length_weight: é•¿åº¦é€‚åº”æƒé‡
        """
        self.keyword_weight = keyword_weight
        self.length_weight = length_weight

        # è‹±æ–‡åœç”¨è¯
        self.stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have',
            'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should',
            'what', 'where', 'when', 'why', 'how', 'who', 'which', 'that', 'this'
        }

    def rerank_documents(self, query: str, documents: List[Any], top_k: int = 5) -> List[Tuple[Any, float]]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            top_k: è¿”å›å‰kä¸ªæ–‡æ¡£

        Returns:
            é‡æ’åºåçš„(æ–‡æ¡£, åˆ†æ•°)å…ƒç»„åˆ—è¡¨
        """
        if not documents:
            return []

        scored_docs = []
        query_keywords = self._extract_keywords(query.lower())

        for doc in documents:
            doc_text = self._extract_text(doc).lower()

            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_score = self._calculate_keyword_score(query_keywords, doc_text)

            # è®¡ç®—é•¿åº¦é€‚åº”åˆ†æ•°
            length_score = self._calculate_length_score(query, doc_text)

            # ç»¼åˆåˆ†æ•°
            final_score = (self.keyword_weight * keyword_score +
                           self.length_weight * length_score)

            scored_docs.append((doc, final_score))

        # æŒ‰åˆ†æ•°æ’åº
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        return scored_docs[:top_k]

    def _extract_text(self, document) -> str:
        """æå–æ–‡æ¡£æ–‡æœ¬"""
        if hasattr(document, 'page_content'):
            return document.page_content
        elif isinstance(document, str):
            return document
        else:
            return str(document)

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å•è¯
        words = re.findall(r'\b\w+\b', text.lower())
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        keywords = [word for word in words if word not in self.stopwords and len(word) > 2]
        return keywords

    def _calculate_keyword_score(self, query_keywords: List[str], doc_text: str) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        if not query_keywords:
            return 0.0

        matches = 0
        for keyword in query_keywords:
            if keyword in doc_text:
                # ç²¾ç¡®å•è¯åŒ¹é…å¾—æ›´é«˜åˆ†
                if f" {keyword} " in doc_text or doc_text.startswith(keyword) or doc_text.endswith(keyword):
                    matches += 1
                else:
                    matches += 0.5  # éƒ¨åˆ†åŒ¹é…

        return matches / len(query_keywords)

    def _calculate_length_score(self, query: str, doc_text: str) -> float:
        """è®¡ç®—é•¿åº¦é€‚åº”åˆ†æ•°"""
        doc_length = len(doc_text)

        # æ ¹æ®æŸ¥è¯¢é•¿åº¦ç¡®å®šç†æƒ³æ–‡æ¡£é•¿åº¦
        query_length = len(query.split())
        if query_length <= 3:
            ideal_length = 150  # çŸ­æŸ¥è¯¢åå¥½çŸ­ç­”æ¡ˆ
        elif query_length <= 8:
            ideal_length = 300  # ä¸­ç­‰æŸ¥è¯¢
        else:
            ideal_length = 500  # é•¿æŸ¥è¯¢åå¥½è¯¦ç»†ç­”æ¡ˆ

        # è®¡ç®—é•¿åº¦åˆ†æ•°ï¼ˆè¶Šæ¥è¿‘ç†æƒ³é•¿åº¦åˆ†æ•°è¶Šé«˜ï¼‰
        length_diff = abs(doc_length - ideal_length)
        max_diff = ideal_length  # æœ€å¤§å·®å¼‚

        length_score = max(0, 1 - (length_diff / max_diff))

        # å¯¹è¿‡çŸ­æ–‡æ¡£è¿›è¡Œæƒ©ç½š
        if doc_length < 50:
            length_score *= 0.5

        return length_score


class ContrastiveReranker:
    """
    å¯¹æ¯”å­¦ä¹ é‡æ’åºå™¨ - åˆ©ç”¨æ­£è´Ÿæ ·æœ¬ä¿¡æ¯
    """

    def __init__(self):
        self.base_reranker = SimpleReranker()

    def rerank_documents(self, query: str, documents: List[Any],
                         contrastive_data: dict = None, top_k: int = 5) -> List[Tuple[Any, float]]:
        """
        ä½¿ç”¨å¯¹æ¯”å­¦ä¹ è¿›è¡Œé‡æ’åº
        """
        if not documents:
            return []

        # å…ˆç”¨åŸºç¡€é‡æ’åºå™¨å¾—åˆ°åŸºç¡€åˆ†æ•°
        base_results = self.base_reranker.rerank_documents(query, documents, len(documents))

        if not contrastive_data:
            return base_results[:top_k]

        # è°ƒæ•´åˆ†æ•°åŸºäºæ­£è´Ÿæ ·æœ¬ä¿¡æ¯
        adjusted_results = []
        negative_samples = contrastive_data.get('negative_samples', [])

        for doc, base_score in base_results:
            doc_text = self.base_reranker._extract_text(doc)

            # æ£€æŸ¥æ˜¯å¦ä¸è´Ÿæ ·æœ¬ç›¸ä¼¼
            is_negative = self._is_similar_to_negatives(doc_text, negative_samples)

            if is_negative:
                # é™ä½è´Ÿæ ·æœ¬çš„åˆ†æ•°
                adjusted_score = base_score * 0.3
            else:
                # ä¿æŒæˆ–è½»å¾®æå‡æ­£æ ·æœ¬åˆ†æ•°
                adjusted_score = base_score * 1.1

            adjusted_results.append((doc, adjusted_score))

        # é‡æ–°æ’åº
        adjusted_results.sort(key=lambda x: x[1], reverse=True)

        return adjusted_results[:top_k]

    def _is_similar_to_negatives(self, doc_text: str, negative_samples: List[dict]) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ä¸è´Ÿæ ·æœ¬ç›¸ä¼¼"""
        doc_clean = doc_text.replace("Question: ", "").replace("Answer: ", "").lower()

        for neg_sample in negative_samples:
            neg_text = neg_sample.get('text', '').replace("Question: ", "").replace("Answer: ", "").lower()

            if neg_text and self._text_similarity(doc_clean, neg_text) > 0.7:
                return True

        return False

    def _text_similarity(self, text1: str, text2: str) -> float:
        """ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—"""
        words1 = set(text1.split())
        words2 = set(text2.split())

        if not words1 or not words2:
            return 0.0

        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))

        return intersection / union if union > 0 else 0.0


# ================== é‡æ’åºå±•ç¤ºå‡½æ•° ==================

def show_retrieved_documents_with_rerank(db, question, contrastive_data=None, reranker=None):
    """
    æ˜¾ç¤ºæ£€ç´¢ç»“æœå’Œé‡æ’åºæ•ˆæœ
    """
    print("\n æ­£åœ¨ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
    retriever = db.as_retriever(search_kwargs={"k": 8})  # æ£€ç´¢8ä¸ªæ–‡æ¡£ç”¨äºé‡æ’åº
    docs = retriever.get_relevant_documents(question)

    if not docs:
        print(" æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
        return []

    print(f" åˆå§‹æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")

    if reranker:
        print(" æ­£åœ¨åº”ç”¨é‡æ’åº...")
        try:
            # åº”ç”¨é‡æ’åº
            if hasattr(reranker, 'rerank_documents'):
                if isinstance(reranker, ContrastiveReranker):
                    reranked_results = reranker.rerank_documents(question, docs, contrastive_data, top_k=5)
                else:
                    reranked_results = reranker.rerank_documents(question, docs, top_k=5)

                print(" é‡æ’åºåçš„ç›¸å…³æ–‡æ¡£ï¼ˆå‰5æ¡ï¼‰ï¼š")
                final_docs = []

                for i, (doc, score) in enumerate(reranked_results, 1):
                    doc_text = doc.page_content

                    # ç®€å•çš„æ­£è´Ÿæ ·æœ¬æ ‡æ³¨
                    doc_type = _identify_doc_type(doc_text, contrastive_data)

                    print(f"\n[é‡æ’åºæ–‡æ¡£ {i}] {doc_type} | åˆ†æ•°: {score:.3f}")
                    print(f"   å†…å®¹: {doc_text[:150]}...")
                    print("-" * 50)

                    final_docs.append(doc)

                # æ˜¾ç¤ºé‡æ’åºæ•ˆæœ
                _show_simple_rerank_stats(len(docs), len(final_docs))

            else:
                final_docs = docs[:5]
                print(" é‡æ’åºå™¨æ¥å£ä¸å…¼å®¹ï¼Œä½¿ç”¨åŸå§‹ç»“æœ")

        except Exception as e:
            print(f" é‡æ’åºå¤±è´¥: {e}")
            final_docs = docs[:5]
    else:
        final_docs = docs[:5]
        print(" æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼ˆå‰5æ¡ï¼‰ï¼š")
        for i, doc in enumerate(final_docs, 1):
            print(f"\n[æ–‡æ¡£ {i}]ï¼š")
            print(f"{doc.page_content[:150]}...")
            print("-" * 50)

    return final_docs


def analyze_retrieval_quality_with_rerank(question, retrieved_docs, contrastive_data):
    """
    ç®€å•çš„æ£€ç´¢è´¨é‡åˆ†æ
    """
    if not retrieved_docs or not contrastive_data:
        return

    positive_count = 0
    negative_count = 0

    for doc in retrieved_docs:
        doc_content = doc.page_content.replace("Question: ", "").replace("Answer: ", "")

        # æ£€æŸ¥æ˜¯å¦ä¸ºè´Ÿæ ·æœ¬
        is_negative = any(
            doc_content.lower() in neg['text'].replace("Question: ", "").replace("Answer: ", "").lower()
            for neg in contrastive_data.get('negative_samples', [])
        )

        if is_negative:
            negative_count += 1
        else:
            positive_count += 1

    total_docs = len(retrieved_docs)
    if total_docs > 0:
        quality_score = (positive_count / total_docs) * 100

        print(f"\næ£€ç´¢è´¨é‡åˆ†æ:")
        print(f"    æ­£æ ·æœ¬æ–‡æ¡£: {positive_count}/{total_docs} ({quality_score:.1f}%)")
        print(f"    è´Ÿæ ·æœ¬æ–‡æ¡£: {negative_count}/{total_docs} ({(negative_count / total_docs) * 100:.1f}%)")

        if quality_score >= 80:
            print(f"    è´¨é‡è¯„ä¼°: ä¼˜ç§€")
        elif quality_score >= 60:
            print(f"    è´¨é‡è¯„ä¼°: è‰¯å¥½")
        else:
            print(f"    è´¨é‡è¯„ä¼°: éœ€è¦æ”¹è¿›")


def demo_reranking_functionality(db, reranker, contrastive_data):
    """
    ç®€å•çš„é‡æ’åºåŠŸèƒ½æ¼”ç¤º
    """
    demo_questions = [
        "What is machine learning?",
        "How does AI work?"
    ]

    print(" é‡æ’åºåŠŸèƒ½æ¼”ç¤º")

    for i, question in enumerate(demo_questions, 1):
        print(f"\n æ¼”ç¤ºé—®é¢˜ {i}: {question}")
        print("-" * 40)

        retriever = db.as_retriever(search_kwargs={"k": 5})
        docs = retriever.get_relevant_documents(question)

        if not docs:
            print(" æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
            continue

        print(" åŸå§‹æ£€ç´¢ç»“æœ:")
        for j, doc in enumerate(docs[:3], 1):
            print(f"   [{j}] {doc.page_content[:80]}...")

        if reranker:
            try:
                if isinstance(reranker, ContrastiveReranker):
                    reranked = reranker.rerank_documents(question, docs, contrastive_data, top_k=3)
                else:
                    reranked = reranker.rerank_documents(question, docs, top_k=3)

                print("\n é‡æ’åºåç»“æœ:")
                for j, (doc, score) in enumerate(reranked, 1):
                    print(f"   [{j}] {doc.page_content[:80]}... (åˆ†æ•°: {score:.2f})")

            except Exception as e:
                print(f" é‡æ’åºæ¼”ç¤ºå¤±è´¥: {e}")


# ================== è¾…åŠ©å‡½æ•° ==================

def _identify_doc_type(doc_text, contrastive_data):
    """è¯†åˆ«æ–‡æ¡£ç±»å‹"""
    if not contrastive_data:
        return "æ— å¯¹æ¯”å­¦ä¹ æ•°æ®"

    doc_clean = doc_text.replace("Question: ", "").replace("Answer: ", "").lower()

    is_negative = any(
        doc_clean in neg['text'].replace("Question: ", "").replace("Answer: ", "").lower()
        for neg in contrastive_data.get('negative_samples', [])
    )

    return " è´Ÿæ ·æœ¬" if is_negative else " æ­£æ ·æœ¬"


def _show_simple_rerank_stats(original_count, final_count):
    """æ˜¾ç¤ºç®€å•çš„é‡æ’åºç»Ÿè®¡"""
    print(f"\n é‡æ’åºç»Ÿè®¡: ä» {original_count} ä¸ªæ–‡æ¡£ä¸­é€‰å‡ºå‰ {final_count} ä¸ª")


def create_reranker(reranker_type="simple", **kwargs):
    """
    åˆ›å»ºé‡æ’åºå™¨

    Args:
        reranker_type: "simple" æˆ– "contrastive"
    """
    if reranker_type == "simple":
        return SimpleReranker(**kwargs)
    elif reranker_type == "contrastive":
        return ContrastiveReranker()
    else:
        raise ValueError(f"æœªçŸ¥çš„é‡æ’åºå™¨ç±»å‹: {reranker_type}")


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™ä¸€äº›ç©ºçš„å‡½æ•°
def test_reranking_performance(db, reranker, contrastive_data):
    """ç®€åŒ–ç‰ˆæ€§èƒ½æµ‹è¯•"""
    print(" é‡æ’åºå™¨å·¥ä½œæ­£å¸¸")


def show_contrastive_learning_stats(contrastive_data):
    """æ˜¾ç¤ºå¯¹æ¯”å­¦ä¹ ç»Ÿè®¡"""
    print(f"\n å¯¹æ¯”å­¦ä¹ æ•°æ®: æ­£æ ·æœ¬ {len(contrastive_data.get('positive_samples', []))} ä¸ª, "
          f"è´Ÿæ ·æœ¬ {len(contrastive_data.get('negative_samples', []))} ä¸ª")

    def show_retrieved_documents_with_rerank(db, question, contrastive_data=None, reranker=None):
        """
        æ˜¾ç¤ºä»RAGçŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°çš„æœ€åŒ¹é…åŸæ–‡ï¼Œåº”ç”¨é‡æ’åºï¼Œå¹¶æ ‡æ³¨æ­£è´Ÿæ ·æœ¬
        """
        print("\n æ­£åœ¨ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
        retriever = db.as_retriever(search_kwargs={"k": 10})  # å…ˆæ£€ç´¢æ›´å¤šæ–‡æ¡£ç”¨äºé‡æ’åº
        docs = retriever.get_relevant_documents(question)

        if not docs:
            print(" æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
            return []

        print(f" åˆå§‹æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")

        # åº”ç”¨é‡æ’åº
        if reranker:
            print(" æ­£åœ¨åº”ç”¨é‡æ’åº...")
            try:
                if hasattr(reranker, 'rerank_documents') and hasattr(reranker, 'weights'):
                    # å¤šé˜¶æ®µé‡æ’åºå™¨
                    rerank_results = reranker.rerank_documents(
                        query=question,
                        documents=docs,
                        contrastive_data=contrastive_data,
                        top_k=5
                    )

                    print(" é‡æ’åºåçš„ç›¸å…³æ–‡æ¡£ï¼ˆå‰5æ¡ï¼‰ï¼š")
                    for i, result in enumerate(rerank_results, 1):
                        doc_text = result.document.page_content

                        # è¯†åˆ«æ–‡æ¡£ç±»å‹
                        doc_type = "æœªçŸ¥"
                        if contrastive_data:
                            is_negative = any(
                                doc_text.replace("Question: ", "").replace("Answer: ", "") in
                                (neg['text'].replace("Question: ", "").replace("Answer: ", ""))
                                for neg in contrastive_data.get('negative_samples', [])
                            )
                            doc_type = " è´Ÿæ ·æœ¬" if is_negative else "ğŸŸ¢ æ­£æ ·æœ¬"

                        # æ˜¾ç¤ºæ’åå˜åŒ–
                        rank_change = result.original_rank - result.new_rank
                        if rank_change > 0:
                            rank_indicator = f" ä»ç¬¬{result.original_rank + 1}ä½ä¸Šå‡åˆ°ç¬¬{result.new_rank + 1}ä½"
                        elif rank_change < 0:
                            rank_indicator = f" ä»ç¬¬{result.original_rank + 1}ä½ä¸‹é™åˆ°ç¬¬{result.new_rank + 1}ä½"
                        else:
                            rank_indicator = f" æ’åæœªå˜ï¼ˆç¬¬{result.new_rank + 1}ä½ï¼‰"

                        print(f"\n[é‡æ’åºæ–‡æ¡£ {i}] {doc_type} | åˆ†æ•°: {result.score:.3f}")
                        print(f"   {rank_indicator}")
                        print(f"   åŸå› : {result.rerank_reason}")
                        print(f"   å†…å®¹: {doc_text[:200]}...")
                        print("-" * 60)

                    # æå–é‡æ’åºåçš„æ–‡æ¡£
                    final_docs = [result.document for result in rerank_results]

                    # æ˜¾ç¤ºé‡æ’åºç»Ÿè®¡
                    show_rerank_statistics(rerank_results)

                else:
                    # ç®€å•é‡æ’åºå™¨
                    reranked_pairs = reranker.rerank_documents(question, docs, top_k=5)
                    print(" é‡æ’åºåçš„ç›¸å…³æ–‡æ¡£ï¼ˆå‰5æ¡ï¼‰ï¼š")

                    final_docs = []
                    for i, (doc, score) in enumerate(reranked_pairs, 1):
                        doc_text = doc.page_content

                        # è¯†åˆ«æ–‡æ¡£ç±»å‹
                        doc_type = "æœªçŸ¥"
                        if contrastive_data:
                            is_negative = any(
                                doc_text.replace("Question: ", "").replace("Answer: ", "") in
                                (neg['text'].replace("Question: ", "").replace("Answer: ", ""))
                                for neg in contrastive_data.get('negative_samples', [])
                            )
                            doc_type = " è´Ÿæ ·æœ¬" if is_negative else " æ­£æ ·æœ¬"

                        print(f"\n[ç®€å•é‡æ’åºæ–‡æ¡£ {i}] {doc_type} | åˆ†æ•°: {score:.3f}")
                        print(f"   å†…å®¹: {doc_text[:200]}...")
                        print("-" * 50)

                        final_docs.append(doc)

            except Exception as e:
                print(f"é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ: {e}")
                final_docs = docs[:5]

                print(" åŸå§‹æ£€ç´¢æ–‡æ¡£ï¼ˆå‰5æ¡ï¼‰ï¼š")
                for i, doc in enumerate(final_docs, 1):
                    doc_text = doc.page_content
                    doc_type = "æœªçŸ¥"
                    if contrastive_data:
                        is_negative = any(
                            doc_text.replace("Question: ", "").replace("Answer: ", "") in
                            (neg['text'].replace("Question: ", "").replace("Answer: ", ""))
                            for neg in contrastive_data.get('negative_samples', [])
                        )
                        doc_type = " è´Ÿæ ·æœ¬" if is_negative else " æ­£æ ·æœ¬"

                    print(f"\n[åŸå§‹æ–‡æ¡£ {i}] {doc_type}ï¼š")
                    print(f"   {doc_text[:200]}...")
                    print("-" * 50)
        else:
            # æ— é‡æ’åºå™¨ï¼Œä½¿ç”¨åŸå§‹ç»“æœ
            final_docs = docs[:5]
            print(" æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼ˆå‰5æ¡ï¼‰ï¼š")
            for i, doc in enumerate(final_docs, 1):
                print(f"\n[æ–‡æ¡£ {i}]ï¼š")
                print(f"{doc.page_content[:200]}...")
                print("-" * 50)

        return final_docs

    def show_rerank_statistics(rerank_results):
        """æ˜¾ç¤ºé‡æ’åºç»Ÿè®¡ä¿¡æ¯"""
        if not rerank_results:
            return

        # è®¡ç®—æ’åå˜åŒ–ç»Ÿè®¡
        rank_changes = [result.original_rank - result.new_rank for result in rerank_results]
        significant_changes = [change for change in rank_changes if abs(change) >= 2]

        avg_score = sum(result.score for result in rerank_results) / len(rerank_results)

        print(f"\n é‡æ’åºç»Ÿè®¡:")
        print(f"    å¹³å‡é‡æ’åºåˆ†æ•°: {avg_score:.3f}")
        print(f"    æ˜¾è‘—æ’åå˜åŒ–æ•°é‡: {len(significant_changes)}")
        print(f"    æœ€å¤§æ’åæå‡: {max(rank_changes) if rank_changes else 0} ä½")
        print(f"   ï¸ æœ€å¤§æ’åä¸‹é™: {abs(min(rank_changes)) if rank_changes else 0} ä½")

        # æŒ‰é‡æ’åºåŸå› ç»Ÿè®¡
        reason_counts = {}
        for result in rerank_results:
            for reason in result.rerank_reason.split('; '):
                if 'é«˜' in reason:
                    key = reason.split('åŒ¹é…')[0] + 'åŒ¹é…'
                    reason_counts[key] = reason_counts.get(key, 0) + 1

        if reason_counts:
            print(f"    ä¸»è¦æå‡å› ç´ : {', '.join(reason_counts.keys())}")

    def analyze_retrieval_quality_with_rerank(question, retrieved_docs, contrastive_data, rerank_results=None):
        """
        åˆ†ææ£€ç´¢è´¨é‡ï¼ŒåŒ…å«é‡æ’åºæ•ˆæœè¯„ä¼°
        """
        if not retrieved_docs or not contrastive_data:
            return

        positive_count = 0
        negative_count = 0

        for doc in retrieved_docs:
            doc_content = doc.page_content.replace("Question: ", "").replace("Answer: ", "")

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…è´Ÿæ ·æœ¬
            is_negative = any(
                doc_content in neg['text'].replace("Question: ", "").replace("Answer: ", "")
                for neg in contrastive_data.get('negative_samples', [])
            )

            if is_negative:
                negative_count += 1
            else:
                positive_count += 1

        total_docs = len(retrieved_docs)
        if total_docs > 0:
            print(f"\n æ£€ç´¢è´¨é‡åˆ†æ:")
            print(f"    æ­£æ ·æœ¬æ–‡æ¡£: {positive_count}/{total_docs} ({positive_count / total_docs * 100:.1f}%)")
            print(f"    è´Ÿæ ·æœ¬æ–‡æ¡£: {negative_count}/{total_docs} ({negative_count / total_docs * 100:.1f}%)")

            # è®¡ç®—æ£€ç´¢è´¨é‡åˆ†æ•°
            quality_score = positive_count / total_docs * 100
            if quality_score >= 80:
                print(f"    æ£€ç´¢è´¨é‡: ä¼˜ç§€ ({quality_score:.1f}åˆ†)")
            elif quality_score >= 60:
                print(f"   ï¸ æ£€ç´¢è´¨é‡: è‰¯å¥½ ({quality_score:.1f}åˆ†)")
            else:
                print(f"    æ£€ç´¢è´¨é‡: éœ€è¦æ”¹è¿› ({quality_score:.1f}åˆ†)")

            # å¦‚æœæœ‰é‡æ’åºç»“æœï¼Œæ˜¾ç¤ºé‡æ’åºæ”¹è¿›æ•ˆæœ
            if rerank_results:
                rerank_positive = 0
                for result in rerank_results:
                    doc_content = result.document.page_content.replace("Question: ", "").replace("Answer: ", "")
                    is_negative = any(
                        doc_content in neg['text'].replace("Question: ", "").replace("Answer: ", "")
                        for neg in contrastive_data.get('negative_samples', [])
                    )
                    if not is_negative:
                        rerank_positive += 1

                rerank_quality = rerank_positive / len(rerank_results) * 100
                improvement = rerank_quality - quality_score

                print(f"    é‡æ’åºåè´¨é‡: {rerank_quality:.1f}åˆ†")
                if improvement > 5:
                    print(f"    é‡æ’åºæ”¹è¿›: +{improvement:.1f}åˆ† (æ˜¾è‘—æå‡)")
                elif improvement > 0:
                    print(f"    é‡æ’åºæ”¹è¿›: +{improvement:.1f}åˆ† (è½»å¾®æå‡)")
                elif improvement < -5:
                    print(f"    é‡æ’åºå½±å“: {improvement:.1f}åˆ† (éœ€è¦è°ƒä¼˜)")
                else:
                    print(f"    é‡æ’åºå½±å“: {improvement:.1f}åˆ† (åŸºæœ¬ä¸å˜)")

        return {
            'positive_count': positive_count,
            'negative_count': negative_count,
            'quality_score': positive_count / total_docs * 100 if total_docs > 0 else 0,
            'retrieved_docs_info': [
                (doc.page_content[:100], "positive" if positive_count > negative_count else "negative")
                for doc in retrieved_docs]
        }
        print(" è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")

    def analyze_retrieval_quality(question, retrieved_docs, contrastive_data):
        """
        åˆ†ææ£€ç´¢è´¨é‡ï¼ŒåŸºäºå¯¹æ¯”å­¦ä¹ æ•°æ®è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£
        """
        if not retrieved_docs or not contrastive_data:
            return

        positive_count = 0
        negative_count = 0
        relevant_docs = []

        for doc in retrieved_docs:
            doc_content = doc.page_content.replace("Question: ", "").replace("Answer: ", "")

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…è´Ÿæ ·æœ¬
            is_negative = any(
                doc_content in neg['text'].replace("Question: ", "").replace("Answer: ", "")
                for neg in contrastive_data.get('negative_samples', [])
            )

            if is_negative:
                negative_count += 1
                relevant_docs.append(("negative", doc_content[:100]))
            else:
                positive_count += 1
                relevant_docs.append(("positive", doc_content[:100]))

        total_docs = len(retrieved_docs)
        if total_docs > 0:
            print(f"\n æ£€ç´¢è´¨é‡åˆ†æ:")
            print(f"    æ­£æ ·æœ¬æ–‡æ¡£: {positive_count}/{total_docs} ({positive_count / total_docs * 100:.1f}%)")
            print(f"    è´Ÿæ ·æœ¬æ–‡æ¡£: {negative_count}/{total_docs} ({negative_count / total_docs * 100:.1f}%)")

            # è®¡ç®—æ£€ç´¢è´¨é‡åˆ†æ•°
            quality_score = positive_count / total_docs * 100
            if quality_score >= 80:
                print(f"    æ£€ç´¢è´¨é‡: ä¼˜ç§€ ({quality_score:.1f}åˆ†)")
            elif quality_score >= 60:
                print(f"    æ£€ç´¢è´¨é‡: è‰¯å¥½ ({quality_score:.1f}åˆ†)")
            else:
                print(f"    æ£€ç´¢è´¨é‡: éœ€è¦æ”¹è¿› ({quality_score:.1f}åˆ†)")

        return {
            'positive_count': positive_count,
            'negative_count': negative_count,
            'quality_score': positive_count / total_docs * 100 if total_docs > 0 else 0,
            'retrieved_docs_info': relevant_docs
        }

    def show_contrastive_learning_stats(contrastive_data):
        """
        æ˜¾ç¤ºå¯¹æ¯”å­¦ä¹ æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        """
        print("\n å¯¹æ¯”å­¦ä¹ æ•°æ®ç»Ÿè®¡:")
        print(f"    æ­£æ ·æœ¬æ•°é‡: {len(contrastive_data.get('positive_samples', []))}")
        print(f"    è´Ÿæ ·æœ¬æ•°é‡: {len(contrastive_data.get('negative_samples', []))}")
        print(f"    ä¸‰å…ƒç»„æ•°é‡: {len(contrastive_data.get('triplets', []))}")

        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹ä¸‰å…ƒç»„
        triplets = contrastive_data.get('triplets', [])
        if triplets:
            print(f"\n å¯¹æ¯”å­¦ä¹ ä¸‰å…ƒç»„ç¤ºä¾‹ (æ˜¾ç¤ºå‰3ä¸ª):")
            for i, triplet in enumerate(triplets[:3], 1):
                print(f"\n   [{i}] é”šç‚¹é—®é¢˜: {triplet['anchor'][:80]}...")
                print(f"        æ­£ç¡®ç­”æ¡ˆ: {triplet['positive'][:60]}...")
                print(f"        é”™è¯¯ç­”æ¡ˆ: {triplet['negative'][:60]}...")